#################################################
## NIFTY: News Information Flow Tracking, Yay! ##
#################################################

Hi! Welcome to NIFTY! NIFTY is a system that finds mutations of a single piece
of information across the daily news cycle. Based on Memetracker, each day, 
the system parses through 3.5 million news articles and 2 million mentioned 
quotes to find the top quote clusters through a process called incremental 
clustering. NIFTY was developed over a 10-week period as part of CURIS, 
Stanford's undergraduate research program. It has a wide variety of 
applications ranging from viewing popular election quotes to seeking the trends
for a particular keyword.

########################
## SETUP INSTRUCTIONS ##
########################

The most important thing you will want to look at is "stdafx.h". There are
several defined variables for storing files that you will want to change for
your machine.

!!! IMPORTANT NOTE !!!!
If you want to run a clean restart of everything, you MUST delete every single
file stored under QBDBC_DIR_DEFAULT. Otherwise, you will get very strange errors.
Trust me, I've been there. In fact, if you ever get incomprehensible errors
(this happens a lot), try checking to make sure you have permissions to edit
files in QBDBC_DIR_DEFAULT, and that any files that you are trying to load
actually exist. This will save you infinit amounts of time!!

##########################
## RUNNING INSTRUCTIONS ##
##########################

The standard way to run the NIFTY pipeline is as follows:
  * Filtering step: takes spinn3r data in and filters it into a document base
    of documents (see lib/doc.h) and quote base of quotes (see lib/quote.h).
    run memefilter using memefilter.cpp, or to run it for the current date,
    use "runmemefilter.sh".
    To run: ./memefilter -date <YYYY-MM-DD> [-qbdb <output directory here>] 
              [-spinn3r <spinner directory here>]
  * Clustering step: takes output from filtering step, builds a graph of quotes
    with edges meaning similar quotes, and deletes edges until subgraphs remain
    as clusters. Files used include lsh, quotegraph, and clustering.cpp. See
    below these bullet points for running instructions.
  * Post-clustering step: not all clusters are "good clusters", so we filter out
    undesirable clusters in postcluster.cpp.
  * Output step: We have a logging mechanism that also prints out web and text
    based output each day. The main classes used are logoutput, printjson, and
    printclusterjson.cpp
    
 Filtering is just run once per each daily file and is pretty insulated. For 
 efficiency and tracking purposes, we have a concept of "incremental clustering"
 where each day's clusters build off previous days' results. Therefore, to begin
 the clustering process we must first "seed" our program with several days' worth
 of data. You can do this using the "memeseed" file:
        ./memeseed -start <startdate> [-window <windowsize>] [-directory <outputdir>]
          [-nolog <just type -nolog and some random characters if you don't want
          logging to occur>]
 "windowsize" refers to the number of days, starting from startdate, that memeseed will
 seed the starting quote and doc bases with.
 
 After memeseed has been run once, you can use memeincrement to build on your previous
 days' clustering output! Simply run memeincrement as follows:
        ./memeincrement -start <startdate> -end <enddate, noninclusive> 
          [-directory <outputdir>] [-nolog <just type -nolog and some random
          characters if you don't want logging to occur>]

Again, end date is noninclusive! So if you want to run memeincrement on 1 dayonly (say
Jan 1), you would run ./memeincrement -start 2012-01-01 -end 2012-01-02.

###################
## SHELL SCRIPTS ##
###################
"runmeme.sh" and "runmemefilter.sh" do cool things. I can't read shell scripts so you're
on your own for this one, heh. Cheers!